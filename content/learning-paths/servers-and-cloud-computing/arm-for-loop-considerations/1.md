---
title: Introduction to Vectorisable Code
weight: 2

### FIXED, DO NOT MODIFY
layout: learningpathall
---

## Introduction

In Arm-based systems, it is crucial to consider the platformâ€™s architecture and compiler capabilities when writing a C++ for loop. By understanding how compilers automatically vectorize code, you can better organize loops to leverage advanced SIMD features for performance. Compiler autovectorization inspects loops at compile time, generating instructions that process multiple data elements in parallel. Depending on compiler flags and data types, the resulting machine code may use different extensions to harness the underlying hardware.

- **NEON**: A 128-bit SIMD extension that processes data in parallel, offering improved performance for single-precision floating-point operations and integer workloads.

- **SVE** (Scalable Vector Extension): Introduces variable-length vectors to provide scalable performance across different Arm implementations, enabling a flexible approach to SIMD

- **SVE2**: Builds upon SVE by adding more instructions for integer, fixed-point, and complex workloads, broadening the range of vectorizable code to enable general data-procssing

While assembly and Arm intrinsics can yield further optimizations, they are beyond this learning path. Instead, we will concentrate on C++ constructs that help the compiler generate efficient vectorized instructions.

## Environment Setup

In this learning path I will be using an AWS Graviton 3 instance based on the Arm Neoverse V1 architecture. This particular instance supports both NEON and SVE. If you are unfamiliar with using cloud instances, please reach out [getting started guide](TODO).

```bash
sudo apt update
sudo apt install g++
```
Please note: There will be slight differences in performance when using difference versions of compilers. 


## Trivial Vectorisable Example

Data-level parallelism (DLP) refers to the capability of modern CPUs, including Arm architectures, to perform operations on multiple data points simultaneously. In practice, this means the compiler can identify loops or repeated calculations on array elements and convert them into a smaller set of vectorized instructions. By grouping data elements, the compiler leverages hardware instructions that operate on multiple values at once, reducing the total number of instruction cycles. This transformation is key to achieving high-performance code on Arm-based systems, where NEON, SVE, and SVE2 extensions are used to efficiently handle tasks that involve large arrays or complex data processing.

Copy and paste the C++ code snippet below into a file named `trivial_vector.cpp`.

```cpp
#include <iostream>
#include <vector>

using namespace std;

void vector_add(const vector<int>& a, const vector<int>& b, vector<int>& c) {
    const int size = a.size();
    for (int i = 0; i < size; ++i) {
        c[i] = a[i] + b[i];
    }
}

int main() {
    const int size = 100;
    vector<int> a(size, 8);
    vector<int> b(size, 2);
    vector<int> c(size, 0);

    vector_add(a, b, c);

    for (int i = 0; i < size; ++i) {
        cout << c[i] << " ";
    }
    cout << endl;

    return 0;
}
```
The snippet above performs a vector add of 2 vectors, a and b of size 100. Notable things to observe are:

- Fixed loop size of 100
- No conditional statements within the loop
- Fixed data type of `int`


### Improving Packing

An obvious optimisation is regarding the data format. In the example above we are us



