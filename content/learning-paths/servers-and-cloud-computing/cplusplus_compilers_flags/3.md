---
title: Finding Supported Neoverse Features
weight: 3

### FIXED, DO NOT MODIFY
layout: learningpathall
---

## Identify the Neoverse Version

To understand which Neoverse version a cloud instance uses check the [Arm partner webpage](https://www.arm.com/partners/aws).

Alternatively, if you already have access to the instance, run the `lscpu` command and observe the underlying Neoverse Architecture under the `Model Name` row.

```output
lscpu | grep -i model
Model name:                           Neoverse-V2
Model:                                1
```
Here you can confirm the AWS`r8g.xlarge` instance, is based on the Neoverse-V2 Arm IP. We will use this instance for the remainder of this learning path.

## Understand Supported CPU Features

Next, to identify the CPU extensions supported by this architecture at runtime we can observe the Linux hardware capabilities (HWCAP) vector. The C++ source code below that reads a specific vector that contains the information.

Copy and paste the c program into a file named, `hw_cap.c`.

```c

#include <stdio.h>
#include <sys/auxv.h>
#include <asm/hwcap.h>

int main()
{
    long hwcaps = getauxval(AT_HWCAP);

    if (hwcaps & HWCAP_AES) {
        printf("AES instructions are available\n");
    } else {
        printf("AES instructions are not available\n");
    }
    if (hwcaps & HWCAP_CRC32) {
        printf("CRC32 instructions are available\n");
    } else {
        printf("CRC32 instructions are not available\n");
    }
    if (hwcaps & HWCAP_PMULL) {
        printf("PMULL/PMULL2 instructions that operate on 64-bit data are available\n");
    } else {
        printf("PMULL/PMULL2 instructions are not available\n");
    }
    if (hwcaps & HWCAP_SHA1) {
        printf("SHA1 instructions are available\n");
    } else {
        printf("SHA1 instructions are not available\n");
    }
    if (hwcaps & HWCAP_SHA2) {
        printf("SHA2 instructions are available\n");
    } else {
        printf("SHA2 instructions are not available\n");
    }
    if (hwcaps & HWCAP_SVE) {
        printf("Scalable Vector Extension (SVE) instructions are available\n");
    } else {
        printf("Scalable Vector Extension (SVE) instructions are not available\n");
    }

    return 0;
}

```

Compile and run with the command below. 

```bash
gcc hw_cap.c -o hw_cap
./hw_cap
```

On Graviton 4, I the output below confirms the scalable vector extensions (SVE) are available.

```output
AES instructions are available
CRC32 instructions are available
PMULL/PMULL2 instructions that operate on 64-bit data are available
SHA1 instructions are available
SHA2 instructions are available
Scalable Vector Extension (SVE) instructions are available
```

For the latest list of all hardware capabilities available for a specific linux kernel version, refer to the `arch/arm/include/uapi/asm/hwcap.h` header file in the Linux Kernel source code.

Further, knowing the width of SVE (Scalable Vector Extension) can be useful for optimizing software performance, as it allows developers to tailor their code to fully utilize the available vector processing capabilities of the hardware. Copy the following C code into a file named `sve_width.c`.  

```c
#include <arm_sve.h>
#include <stdio.h>

int main() {
    int sve_width = svcntb();
    printf("SVE vector length: %d bytes\n", sve_width);
    return 0;
}
```

Compile with the following command. 

```bash
g++ sve_width.c -o sve_width -mcpu=neoverse-v2
```

This shows that the Neoverse-V2 based Graviton 4 instance has a SVE width of 8 bytes (128 bits).

```output
SVE vector length: 16 bytes
```

## Supported Compiler Features

Fortunately, the g++ compiler will automatically identify the host systems capability. The `-###` argument can be used to show the full options used when compiling. 

If the host is the same platform you are compiling for, you can observe which CPUs are potential targets for your command with the following g++ command. 

```output
g++ -E -mcpu=help -xc /dev/null
cc1: note: valid arguments are: cortex-a34 cortex-a35 cortex-a53 cortex-a57 cortex-a72 cortex-a73 thunderx thunderxt88p1 thunderxt88 octeontx octeontx81 octeontx83 thunderxt81 thunderxt83 ampere1 ampere1a emag xgene1 falkor qdf24xx exynos-m1 phecda thunderx2t99p1 vulcan thunderx2t99 cortex-a55 cortex-a75 cortex-a76 cortex-a76ae cortex-a77 cortex-a78 cortex-a78ae cortex-a78c cortex-a65 cortex-a65ae cortex-x1 cortex-x1c **neoverse-n1** ares neoverse-e1 octeontx2 octeontx2t98 octeontx2t96 octeontx2t93 octeontx2f95 octeontx2f95n octeontx2f95mm a64fx tsv110 thunderx3t110 neoverse-v1 zeus neoverse-512tvb saphira cortex-a57.cortex-a53 cortex-a72.cortex-a53 cortex-a73.cortex-a35 cortex-a73.cortex-a53 cortex-a75.cortex-a55 cortex-a76.cortex-a55 cortex-r82 cortex-a510 cortex-a710 cortex-a715 cortex-x2 cortex-x3 neoverse-n2 cobalt-100 neoverse-v2 grace demeter generic
```

Comparing to when using `g++9` we can see there are fewer CPU targets to optimise for as recently released CPUs are omitted, for example the Neoverse V2. 

```
g++-9 -E -mcpu=help -xc /dev/null
cc1: note: valid arguments are: cortex-a35 cortex-a53 cortex-a57 cortex-a72 cortex-a73 thunderx thunderxt88p1 thunderxt88 octeontx octeontx81 octeontx83 thunderxt81 thunderxt83 emag xgene1 falkor qdf24xx exynos-m1 phecda thunderx2t99p1 vulcan thunderx2t99 cortex-a55 cortex-a75 cortex-a76 ares neoverse-n1 neoverse-e1 a64fx tsv110 zeus neoverse-v1 neoverse-512tvb saphira neoverse-n2 cortex-a57.cortex-a53 cortex-a72.cortex-a53 cortex-a73.cortex-a35 cortex-a73.cortex-a53 cortex-a75.cortex-a55 cortex-a76.cortex-a55 generic
```




