---
title: Try an example application
weight: 5

### FIXED, DO NOT MODIFY
layout: learningpathall
---

## Understand your goal

If you intend for your application to be portable across a variety of Arm servers, you should select a target architecture with, `-march=` with a value matching the lowest Arm architecture among the set of systems you plan to use. This is enabled by the backwards compatibility of the Arm architecture. 

If you are running your C++ application in a memory constrained environment, such as a containerized environment, you should optimize for size. 

If you're building an application to get the highest performance on a specific processor, such as AWS Graviton 4 (Arm Neoverse V2), you should consider  specifying the system using the `-mcpu` flag. 

While these are general guidelines, you should experiment with the various optimization settings. 

## What is a vectorizable loop?

Use an editor to copy and paste the C++ code below into a file named `vectorizable_loop.cpp`. 

The code initializes a vector of 1 million elements, doubles each element, and stores the result in the same vector. This is repeated 5 times to calculate the average runtime. 

```cpp
#include <vector>
#include <chrono>
#include <iostream>
#include <unistd.h> // for getpid()

void vectorizable_loop(std::vector<int>& data) {
    double total_elapsed_time = 0.0;
    const int iterations = 5;

    for (int iter = 0; iter < iterations; ++iter) {
        auto start = std::chrono::high_resolution_clock::now();
        
        for (size_t i = 0; i < data.size(); ++i) {
            data[i] *= 2;
        }
        
        auto end = std::chrono::high_resolution_clock::now();
        std::chrono::duration<double> elapsed = end - start;
        total_elapsed_time += elapsed.count();
        std::cout << "Elapsed time for iteration " << iter + 1 << ": " << elapsed.count() << " seconds" << std::endl;
    }

    double average_elapsed_time = total_elapsed_time / iterations;
    std::cout << "Average elapsed time: " << average_elapsed_time << " seconds" << std::endl;
}

int main() {
    const size_t size = 100000000; // 100 million elements
    std::vector<int> data(size, 1);

    std::cout << "Process ID (PID): " << getpid() << std::endl;

    vectorizable_loop(data);

    return 0;
}
```

## Use different versions of the G++ compiler

Compare compiler versions by building `vectorizable_loop.cpp` with the same arguments, but different compiler versions. 

Run the commands below to use the default g++ compiler (13.3) and then g++ version installed earlier 9 on the same code with the same flags. 

```bash
g++ vectorizable_loop.cpp -o vectorizable_loop_gcc_13
g++-9 vectorizable_loop.cpp -o vectorizable_loop_gcc_9
./vectorizable_loop_gcc_13
./vectorizable_loop_gcc_9
```

In this simple example you observe about 20% speed improvement moving from version 9 to version 13. 

The output is shown below:

```output
// gcc v.13
Process ID (PID): 5130
Elapsed time for iteration 1: 0.291529 seconds
Elapsed time for iteration 2: 0.291689 seconds
Elapsed time for iteration 3: 0.291874 seconds
Elapsed time for iteration 4: 0.292063 seconds
Elapsed time for iteration 5: 0.29209 seconds
Average elapsed time: 0.291849 seconds
// gcc v.9

Process ID (PID): 5131
Elapsed time for iteration 1: 0.363335 seconds
Elapsed time for iteration 2: 0.362316 seconds
Elapsed time for iteration 3: 0.361944 seconds
Elapsed time for iteration 4: 0.36257 seconds
Elapsed time for iteration 5: 0.362911 seconds
Average elapsed time: 0.362615 seconds
```

You see that newer compiler versions can impact performance. 

## Target performance

Another way to observe compiler impact is the optimization level. 

Compile the application with three different optimization levels:

```bash
g++ -O1 vectorizable_loop.cpp -o level_1
g++ -O2 vectorizable_loop.cpp -o level_2
g++ -O3 vectorizable_loop.cpp -o level_3
```

Running the 3 output binaries we observe a significant elapsed time improvement with minimal change in file size and compile time. Please note that larger code bases may see larger output binary sizes and compilation time. 

```bash
./level_1
./level_2
./level_3
```

The output from each executable is below:

```output
Average elapsed time: 0.0526484 seconds
Average elapsed time: 0.0420332 seconds
Average elapsed time: 0.0155661 seconds
```

Here we can observe a notable performance speed up from using higher levels of optimizations.

Please Note: To understand which lower level optimization are used by `-O1`, `-O2` and `-O3` we can use the `g++ <optimization level> -Q --help=optimizers` command. 


### Understanding what was optimized 

Naturally, the next question is to understand which part of your source code was optimized between the outputs above. Full optimization reports generated by compilers like GCC provide a detailed tree of reports through various stages of the optimization process. For beginners, these reports can be overwhelming due to the sheer volume of information they contain, covering every aspect of the code's transformation and optimization. 

For a more manageable overview, you can enable basic optimization information (`opt-info`) reports using specific arguments such as `-fopt-info-vec`, which focuses on vectorization optimizations. The `-fopt-info` flag can be customized by changing the info bit to target different types of optimizations, making it easier to pinpoint specific areas of interest. 

First, to see what part of our source code was optimized between levels 1 and 2 we can run the following commands to see if our vectorizable loop was indeed vectorised. 

```bash
g++ -O1 vectorizable_loop.cpp -o level_1 -fopt-info-vec
```

Running the `-O1` flag led showed no terminal output indicating no vectorisation was performed. Next, run the command below with the `-O2` flag.

```bash
g++ -O2 vectorizable_loop.cpp -o level_2 -fopt-info-vec
```

This time the `-O2` flag enables our loop to be vectorized as can be seen from the output below.

```output
vectorizable_loop.cpp:13:30: optimized: loop vectorized using 16 byte vectors
/usr/include/c++/13/bits/stl_algobase.h:930:22: optimized: loop vectorized using 16 byte vectors
```

To see what optimizations were performed and missed between level 2 and level 3, we could direct the terminal output from all optimizations (`-fopt-info`) to a text file with the commands below. 

```bash
g++ -O2 vectorizable_loop.cpp -o level_2 -fopt-info 2>&1 | tee level2.txt
g++ -O3 vectorizable_loop.cpp -o level_3 -fopt-info 2>&1 | tee level3.txt
```

Comparing the outputs between different levels can highlight where in your source code opportunities to optimize code where missed, for example with the `diff` command. This can help you write source code that is more likely to be optimized. However, source code modifications are out of scope for this learning path and we will leave it to the reader to dive into the differences if they wish to learn more. 


## Target balanced performance

For balanced performance, AWS recommends using the `-mcpu=neoverse-512tvb` option. The value `neoverse-512tvb` instructs GCC to optimize for Neoverse cores that support SVE and have a vector bandwidth of 512 bits per cycle. 

This option directs GCC to target Neoverse cores capable of executing four 128-bit Advanced SIMD arithmetic instructions per cycle, as well as an equivalent number of SVE arithmetic instructions per cycle (two for 256-bit SVE, four for 128-bit SVE). This tuning is more general than optimizing for a specific core like Neoverse V1, but more specific than the default tuning options.

There are numerous options available for G++ that impact code performance and size. A basic understanding of these helps helps you to pick the best options for your application. 

