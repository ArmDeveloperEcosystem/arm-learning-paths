---
title: Source Code Example
weight: 5

### FIXED, DO NOT MODIFY
layout: learningpathall
---

## Defining the goal

If you intend your application to be portable across a variety of Arm architecture versions, selecting a target architecture with, `-march=` with a the value mapped to the lowest Arm architecture in your deployment fleet. The is enabled by the backwards compatibility of the Arm architecture. If running your C++ application in a memory constrained environment, for example in a containerised environment, you may wish to consider optimising for size. 

If you're building to be performant on a specific CPU, as in our case we are building to run natively on an AWS Graviton 4 instance (Arm Neoverse V2), we recommend specifying the system using the `-mcpu` flag. 

## Vectorizable Loop

Copy and paste the following C++ snippet into a file called `vectorizable_loop.cpp`. The naive snippet below initialiases a vector of 1 million elements and doubles each element, storing the result in the same vector. This is repeated 5 times to caculate the average runtime. This naive loop with be autovectorized by the compiler.

```c++
#include <vector>
#include <chrono>
#include <unistd.h> // for getpid()

void vectorizable_loop(std::vector<int>& data) {
    double total_elapsed_time = 0.0;
    const int iterations = 5;

    for (int iter = 0; iter < iterations; ++iter) {
        auto start = std::chrono::high_resolution_clock::now();
        
        for (size_t i = 0; i < data.size(); ++i) {
            data[i] *= 2;
        }
        
        auto end = std::chrono::high_resolution_clock::now();
        std::chrono::duration<double> elapsed = end - start;
        total_elapsed_time += elapsed.count();
        std::cout << "Elapsed time for iteration " << iter + 1 << ": " << elapsed.count() << " seconds" << std::endl;
    }

    double average_elapsed_time = total_elapsed_time / iterations;
    std::cout << "Average elapsed time: " << average_elapsed_time << " seconds" << std::endl;
}

int main() {
    const size_t size = 100000000; // 100 million elements
    std::vector<int> data(size, 1);

    std::cout << "Process ID (PID): " << getpid() << std::endl;

    vectorizable_loop(data);

    return 0;
}

```

## Using different versions of the g++ compiler

As a trivial example we will compile `vectorizable_loop.cpp` with the same arguments. 

```bash
g++ vectorizable_loop.cpp -o vectorizable_loop_gcc_13
g++-9 vectorizable_loop.cpp -o vectorizable_loop_gcc_9
./vectorizable_loop_gcc_13
./vectorizable_loop_gcc_9
```
In this naive and trivial example we observe ~19% speed improvement moving from version 9 to version 13. In reality, large code bases with multiple files are more likely to observe runtime and memory improvements. 

```output
// gcc v.13
Process ID (PID): 5130
Elapsed time for iteration 1: 0.291529 seconds
Elapsed time for iteration 2: 0.291689 seconds
Elapsed time for iteration 3: 0.291874 seconds
Elapsed time for iteration 4: 0.292063 seconds
Elapsed time for iteration 5: 0.29209 seconds
Average elapsed time: 0.291849 seconds
// gcc v.9

Process ID (PID): 5131
Elapsed time for iteration 1: 0.363335 seconds
Elapsed time for iteration 2: 0.362316 seconds
Elapsed time for iteration 3: 0.361944 seconds
Elapsed time for iteration 4: 0.36257 seconds
Elapsed time for iteration 5: 0.362911 seconds
Average elapsed time: 0.362615 seconds
```

## Targeting Performance

```bash
g++ -O1 vectorizable_loop.cpp -o level_1
g++ -O2 vectorizable_loop.cpp -o level_2
g++ -O3 vectorizable_loop.cpp -o level_3
```

Running the 3 output binaries we observe a significant elapsed time improvement with minimal change in file size and compile time. Please note that larger code bases may see larger output binary sizes and compilation time. 

```output
./level_1
Average elapsed time: 0.0526484 seconds
./level_2
Average elapsed time: 0.0420332 seconds
./level_3
Average elapsed time: 0.0155661 seconds
```

## Understanding what optimisations were used

Naturally, the next question is to understand which part of your source code was optimised. Full optimization reports generated by compilers like GCC provide a detailed tree of reports through various stages of the optimization process. These reports can be overwhelming due to the sheer volume of information they contain, covering every aspect of the code's transformation and optimization. 

For a more manageable overview, you can enable basic optimization reports using specific arguments such as -fopt-info-vec, which focuses on vectorization optimizations. The -fopt-info flag can be customized by changing the info bit to target different types of optimizations, making it easier to pinpoint specific areas of interest without being inundated with data. 

Applying this to the following command we can see that there is no vector optimisation with the `-O1` optimisation level. 

```
g++ -O1 vectorizable_loop.cpp -o level_1 -fopt-info-vec
```

```
g++ -O2 vectorizable_loop.cpp -o level_1 -fopt-info-vec
vectorizable_loop.cpp:13:30: optimized: loop vectorized using 16 byte vectors
/usr/include/c++/13/bits/stl_algobase.h:930:22: optimized: loop vectorized using 16 byte vectors
```
However the same command with the `-O2` optimistaion level we observe line 13, column 30 of our source code was optimised. 

## Targeting Balanced Performance

On AWS, for balanced performance they recommend using the `-mcpu=neoverse-512tvb` option. The value ‘neoverse-512tvb’ instructs GCC to optimize for Neoverse cores that support SVE and have a vector bandwidth of 512 bits per cycle. Essentially, this option directs GCC to target Neoverse cores capable of executing four 128-bit Advanced SIMD arithmetic instructions per cycle, as well as an equivalent number of SVE arithmetic instructions per cycle (two for 256-bit SVE, four for 128-bit SVE). This tuning is more general than optimizing for a specific core like Neoverse V1, but more specific than the default tuning options.


