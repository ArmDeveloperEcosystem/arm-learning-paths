---
name: generate_report
description: Generate comprehensive markdown report from profiling results, combining latency metrics and operator-level analysis. Compares SME2-on vs SME2-off performance with statistical analysis. Use when creating final reports, documenting profiling results, or sharing performance analysis with stakeholders.
---

# Skill: Generate Report

**Purpose**: Generate comprehensive markdown report from profiling results

**When to use**: 
- After `analyze_results` completes successfully
- When creating final documentation of profiling results
- When sharing performance analysis with stakeholders
- When comparing SME2-on vs SME2-off performance in a formatted report

## Overview

This skill generates a comprehensive markdown report from **CSV files** that were generated from ETDump files. The workflow is:

1. **ETDump files** (from `run_profiling`) → 
2. **CSV files** (generated by `analyze_results.py` using `etdump_to_csv.py`) → 
3. **Report** (generated by this script from CSV files)

**Why CSV files?**
- **Verifiability**: CSV files are human-readable and can be inspected manually
- **Reproducibility**: Reports can be regenerated from CSV files without re-running profiling
- **Transparency**: Raw data is available for independent verification
- **Trust**: You can verify the numbers in the report by checking the CSV files

The report combines:
- **Latency metrics** from timeline CSV files (per-run operator timing)
- **Operator analysis** from stats CSV files (aggregated operator statistics)
- **Metadata** from `manifest.json` (run configuration, ExecuTorch version, config path, runner paths)
- **Device information** (automatically detected from system)

The report provides a clear comparison between SME2-on and SME2-off configurations, showing latency improvements, operator category breakdowns, and statistical summaries.

**Key features**:
- Statistical analysis (mean, median, min, max, standard deviation) from CSV timeline data
- Percentage improvement calculations
- Operator category breakdowns from CSV stats files
- Per-experiment comparisons
- Complete traceability (config path, runner paths, device info)

**Prerequisites**:
- `.venv/` activated
- **CSV files exist** (generated by `analyze_results.py` in same directory as ETDump files)
- `manifest.json` exists (for config path and metadata)
- Config JSON file accessible (for runner path extraction)

**Important**: 
- No PTE file required - script reads from CSV files and manifest.json
- CSV files should be in same directory as ETDump files (e.g., `runs/mac/mac_sme2_on/`)
- Run `05_analyze_results.md` first to generate CSV files

## Steps

### 1. Activate Virtual Environment

```bash
source .venv/bin/activate
```

### 2. Generate CSV Files (if not already done)

```bash
# Ensure CSV files exist (generated by analyze_results.py)
python model_profiling/scripts/analyze_results.py \
  --run-dir model_profiling/out_toy_cnn/runs/mac
```

This creates CSV files in the same directory as ETDump files (e.g., `runs/mac/mac_sme2_on/`).

### 3. Generate Report

```bash
python model_profiling/scripts/generate_report.py \
  --run-dir model_profiling/out_toy_cnn/runs/mac \
  --out model_profiling/out_toy_cnn/runs/mac/report.md
```

**Note**: 
- No PTE file required - script reads from CSV files and manifest.json
- Report is generated from CSV files (not ETDump directly)
- CSV files are in same directory as ETDump files for easy verification

**Default behavior**: If `--out` is not specified, report is written to `<run-dir>/report.md`

**Note**: The `--model` parameter is optional but recommended. It enables:
- Model name extraction
- ETRecord lookup for operator name enrichment
- Better traceability

**Example with custom title**:
```bash
python model_profiling/scripts/generate_report.py \
  --run-dir model_profiling/out_toy_cnn/runs/mac \
  --out model_profiling/out_toy_cnn/runs/mac/report.md \
  --model model_profiling/out_toy_cnn/artifacts/toy_cnn_xnnpack_fp16.pte \
  --title "Toy CNN Profiling Report"
```

### 4. Review Report

```bash
# View report
cat model_profiling/out_toy_cnn/runs/mac/report.md

# Or open in markdown viewer
open model_profiling/out_toy_cnn/runs/mac/report.md
```

**Verification**:

```bash
# Check report exists
test -f model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Report exists"

# Check report is non-empty
test -s model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Report is non-empty"

# Check CSV files exist (in same directory as ETDump files)
test -f model_profiling/out_toy_cnn/runs/mac/mac_sme2_on/*_timeline.csv && echo "✓ Timeline CSV files exist"
test -f model_profiling/out_toy_cnn/runs/mac/mac_sme2_on/*_ops_stats.csv && echo "✓ Stats CSV files exist"

# Check report contains critical traceability information
grep -q "Config File" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Contains config file path"
grep -q "Runner Paths" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Contains runner paths"
grep -q "Device Information" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Contains device information"
grep -q "CSV Files Analyzed" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Report acknowledges CSV source"

# Check report contains key sections
grep -q "Latency Comparison" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Contains latency comparison"
grep -q "Operator Category Breakdown" model_profiling/out_toy_cnn/runs/mac/report.md && echo "✓ Contains category breakdown"
```

**Expected outputs**:
- `<run-dir>/report.md` - Comprehensive markdown report with:
  - **Metadata section** (model, config file path, runner paths, device info, ExecuTorch SHA)
  - **Latency comparison table** (SME2-on vs SME2-off) - **derived from CSV timeline files**
  - **Operator category breakdown** - **derived from CSV stats files**
  - **Per-experiment analysis**
  - **Kernel hints** (if available)
  - **Summary section**
  - **Generated Artifacts** section listing CSV files used

**Data Source**: Report is generated from CSV files in `<run-dir>/csv/`, not directly from ETDump. This ensures verifiability and reproducibility.

## Report Structure

The generated report includes:

1. **Metadata** (Critical for traceability):
   - Model name and path
   - **Config file path** (for full experiment traceability)
   - **Runner paths** (SME2-on and SME2-off runner binaries used)
   - **Device information** (architecture, OS, OS version)
   - ExecuTorch SHA and status
   - ETRecord availability
   - ETDump files analyzed
2. **Latency Comparison**: Statistical comparison table with improvement percentages
3. **Operator Category Breakdown**: Total and per-experiment category timing
4. **Kernel Hints**: Kernel selection information (if trace-enabled runs were used)
5. **Generated Artifacts**: List of files created during profiling
6. **Summary**: High-level performance summary

**Critical Information Included**:
- **Config file path**: Enables full experiment reproduction
- **Runner paths**: Documents which binaries were used (critical for version tracking)
- **Device info**: Architecture, OS, and version for reproducibility

## Interpreting the Report

### Latency Comparison Table

| Metric | SME2-On | SME2-Off | Improvement |
|--------|---------|----------|-------------|
| Median Latency | X.XXX ms | Y.YYY ms | +Z.ZZ% |

**Improvement calculation**: `(SME2-Off - SME2-On) / SME2-Off * 100`
- **Positive values** → SME2 is faster (improvement)
- **Negative values** → SME2 is slower (regression)
- **Zero** → No significant difference

### Category Breakdown

Shows time spent in each operator category:
- **Convolution**: Conv2d, DepthwiseConv operations
- **GEMM**: Linear, MatMul operations
- **Data Movement**: Transpose, Permute, Copy operations
- **Elementwise**: Add, Mul, ReLU, etc.
- **Other**: Everything else

**Key insight**: After SME2 accelerates CONV/GEMM, Data Movement often becomes the bottleneck.

## Advanced: Kernel View Generation

For trace-enabled runs, you can generate kernel comparison tables:

```bash
# Generate kernel view (requires xnntrace logs or CSV files)
python model_profiling/tools/generate_kernel_view.py \
  --sme2-on <path-to-sme2-on-xnntrace.log> \
  --sme2-off <path-to-sme2-off-xnntrace.log> \
  --out kernel_view.md
```

This creates a markdown table showing which kernels are present in each configuration.

## Failure Handling

| Issue | Symptom | Fix |
|-------|---------|-----|
| **No CSV files found** | `FileNotFoundError: No CSV files found` | Run `05_analyze_results.md` first to generate CSV files from ETDump |
| **CSV files missing** | Report cannot find timeline or stats CSV | Check CSV files exist in same directory as ETDump files (e.g., `runs/mac/mac_sme2_on/`) |
| **ETDump parsing errors** | Errors when generating CSV files | Check ETDump files are valid, verify ExecuTorch Inspector is available |
| **Runner paths not found** | Status shows "⚠️ Not found" | Verify runner paths in config file are correct, check runners were built |
| **Empty report** | Report file exists but is empty | Check CSV files are valid, verify run directory path |
| **No improvement shown** | All improvements are 0% | Verify SME2-on and SME2-off experiments ran correctly, check runner paths |
| **Config file not found** | Config file path in manifest is invalid | Verify config file exists at the path specified in manifest.json |
| **CSV parsing errors** | Errors reading CSV files | Verify CSV files are valid, check they were generated correctly by analyze_results.py |

## Common Workflows

### Workflow 1: Standard Report Generation

```bash
# 1. Run profiling
python model_profiling/scripts/mac_pipeline.py --config model_profiling/configs/my_run.json

# 2. Analyze results (generates CSV files from ETDump)
python model_profiling/scripts/analyze_results.py \
  --run-dir model_profiling/out_<model>/runs/mac

# 3. Verify CSV files were generated (in same directory as ETDump files)
ls -lh model_profiling/out_<model>/runs/mac/*/*.csv

# 4. Generate report (reads from CSV files and manifest.json)
python model_profiling/scripts/generate_report.py \
  --run-dir model_profiling/out_<model>/runs/mac
```

### Workflow 2: Report with Custom Title

```bash
python model_profiling/scripts/generate_report.py \
  --run-dir model_profiling/out_<model>/runs/mac \
  --out model_profiling/out_<model>/runs/mac/custom_report.md \
  --title "My Model Performance Analysis"
```

### Workflow 3: Include Kernel Analysis

```bash
# 1. Generate standard report
python model_profiling/scripts/generate_report.py --run-dir model_profiling/out_<model>/runs/mac

# 2. Generate kernel view (if trace-enabled runs available)
python model_profiling/tools/generate_kernel_view.py \
  --sme2-on model_profiling/out_<model>/runs/mac/<exp>_sme2_on/xnntrace.log \
  --sme2-off model_profiling/out_<model>/runs/mac/<exp>_sme2_off/xnntrace.log \
  --out model_profiling/out_<model>/runs/mac/kernel_view.md

# 3. Manually combine reports if needed
```

## Best Practices

- **Generate CSV files first** - Run `analyze_results.py` to create CSV files before generating report
- **Verify CSV files exist** - Check `runs/<platform>/csv/` directory contains timeline and stats CSV files
- **Inspect CSV files manually** - CSV files are human-readable and can be verified independently
- **Use descriptive titles** - Makes reports easier to identify later
- **Version control reports and CSV files** - Commit both report.md and CSV files for full traceability
- **Include metadata** - Report automatically includes ExecuTorch SHA, config path, runner paths for reproducibility
- **Review summary section** - Quick way to see if SME2 is providing benefits

## Implementation Checklist

- [ ] Virtual environment activated
- [ ] **CSV files exist** (generated by `analyze_results.py` in same directory as ETDump files)
- [ ] `manifest.json` exists (for config path and runner paths)
- [ ] Report generation script executed successfully (no PTE file required)
- [ ] report.md exists and is non-empty
- [ ] Report contains **config file path** (critical for traceability)
- [ ] Report contains **runner paths** (both relative and full paths)
- [ ] Report contains **device information** (architecture, OS, version)
- [ ] Report contains latency comparison section (derived from CSV timeline files)
- [ ] Report contains operator category breakdown (derived from CSV stats files)
- [ ] Report lists CSV files in Generated Artifacts section
- [ ] Summary section shows improvement/regression status

**References**:
- Report generation script: `model_profiling/scripts/generate_report.py`
- CSV generation tool: `model_profiling/tools/etdump_to_csv.py` (called by analyze_results.py)
- Analysis script: `model_profiling/scripts/analyze_results.py` (generates CSV files)
- Kernel view tool: `model_profiling/tools/generate_kernel_view.py`
- Pipeline script: `model_profiling/scripts/mac_pipeline.py`

**Assets**:
- `model_profiling/scripts/generate_report.py` - Main report generation script (reads from CSV files)
- `model_profiling/scripts/analyze_results.py` - Generates CSV files from ETDump
- `model_profiling/tools/etdump_to_csv.py` - Converts ETDump to CSV format
- `model_profiling/tools/generate_kernel_view.py` - Kernel comparison table generator

**Next skill**: None (this is the final step in the profiling workflow)
