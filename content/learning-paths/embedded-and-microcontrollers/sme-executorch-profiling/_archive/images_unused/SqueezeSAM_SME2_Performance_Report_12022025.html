<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SqueezeSAM FP16 ARM SME2 Performance Analysis Report</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; margin-bottom: 30px; text-align: center; }
        h2 { color: #34495e; border-bottom: 2px solid #ecf0f1; padding-bottom: 5px; margin-top: 30px; margin-bottom: 15px; }
        h3 { color: #2c3e50; margin-top: 25px; margin-bottom: 10px; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #3498db; color: white; }
        code { background-color: #f1f2f6; padding: 2px 6px; border-radius: 3px; font-family: 'Monaco', 'Menlo', monospace; }
        pre { background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }
        ul, ol { margin: 10px 0; padding-left: 30px; }
        li { margin: 5px 0; }
        blockquote { border-left: 4px solid #3498db; margin: 20px 0; padding: 10px 20px; background-color: #f8f9fa; }
    </style>
</head>
<body>
<h1>SqueezeSAM FP16 ARM SME2 Performance Analysis Report</h1>
<blockquote>
<p><strong>Date</strong>: November 14, 2025<br />
<strong>Models</strong>: <code>squeeze_sam_xnnpack_fp16.pte</code> (bicubic) and <code>squeeze_sam_xnnpack_fp16_bilinear.pte</code> (optimized for executorch)</p>
</blockquote>
<hr />
<h2><strong>1. TLDR: Summary</strong></h2>
<p>ARM SME2 delivers significant speedups for SqueezeSAM FP16 on both macOS and Android, with <strong>conv2d-heavy workloads</strong> benefiting from the new FP16 iGEMM SME2 kernels. SqueezeSAM is a model used in Meta's family of applications that support Instagram's cutout features. </p>
<h3><strong>Overall Performance Impact</strong></h3>
<p><strong>Combined Optimization (SME2 + Operator Optimization)</strong>: The optimized bilinear model with SME2 achieves dramatic speedups:<ul>
<li><strong>Android (Vivo X300, 1 CPU)</strong>:<strong>1,800ms → 303ms</strong> (5.9× faster) — from original bicubic model without SME2 to optimized bilinear model with SME2</li>
<li><strong>macOS (Apple M4, 1 CPU)</strong>:<strong>545ms → 73ms</strong> (7.5× faster) — from original bicubic model without SME2 to optimized bilinear model with SME2</li>
</ul></p>
<h3><strong>Performance Breakdown</strong></h3>
<p><strong>1. SME2 Kernel Acceleration</strong>: SME2 provides <strong>3.73× speedup on Android</strong> (1,132ms → 303ms) and <strong>4.41× speedup on macOS</strong> (320ms → 73ms) for the optimized bilinear model (1 CPU). This speedup comes from:<ul>
<li><strong>CONV operations (conv2d)</strong>:<strong>8-9× speedup</strong> with FP16 iGEMM SME2 kernels from <a href="https://github.com/google/XNNPACK/pull/8687">XNNPACK PR #8687</a></li>
<li><strong>GEMM operations</strong>:<strong>4-6× speedup</strong> with FP16 GEMM SME2 kernels</li>
<li>The model is <strong>conv2d-heavy</strong>, making iGEMM performance critical</li>
</ul></p>
<p><strong>2. Operator Optimization</strong>: Operator-level performance profiling led to replacing bicubic interpolation with bilinear, which eliminates portable fallback kernels and enables delegated operators:<ul>
<li><strong>Android</strong>:<strong>1.84× speedup</strong> (1,800ms → 1,132ms, SME2-Off baseline)</li>
<li><strong>macOS</strong>:<strong>1.70× speedup</strong> (545ms → 320ms, SME2-Off baseline)</li>
</ul></p>
<blockquote>
<p><strong>Note</strong>: Exporting SqueezeSAM to FP16 ExecuTorch format required addressing FP16 accumulation overflow, enabling iGEMM kernel logging, and optimizing the model by replacing bicubic with bilinear interpolation. See <a href="#appendix-a-fp16-export--source-code-adjustments">Appendix A: FP16 Export &amp; Source Code Adjustments</a> for details.</p>
</blockquote>
<p><strong>Key Insight</strong>: SqueezeSAM's performance gains come from <strong>two sources</strong>: (1) <strong>SME2 kernels</strong> (iGEMM for conv2d, GEMM for matmul) providing 3.73-4.41× overall speedup, and (2) <strong>model operator optimization</strong> (bilinear vs bicubic) eliminating portable kernel execution. The combined effect multiplies the benefits, achieving <strong>5.9× total speedup on Android</strong> and <strong>7.5× on macOS</strong>.</p>
<hr />
<h2><strong>2. Model Context &amp; Architecture</strong></h2>
<h3><strong>Model Origin</strong></h3>
<p>SqueezeSAM is a lightweight, UNet-based variant of the Segment Anything Model (SAM) designed for on-device interactive segmentation. The original architecture is described in the SqueezeSAM paper as a fully convolutional UNet with a small transformer block at the bottleneck and a SAM-style mask head trained from scratch on SA-1B.</p>
<p>For this work, the model implementation was sourced from the squeeze_sam module in the EfficientSAM GitHub repository at commit <a href="https://github.com/yformer/EfficientSAM/blob/5a573e1001018641b25c928a59f0769a623fce92/squeeze_sam/squeeze_sam.py"><code>5a573e1001018641b25c928a59f0769a623fce92</code></a> and integrated into ExecuTorch as an example model.</p>
<h3><strong>Model Architecture</strong></h3>
<p>SqueezeSAM uses a UNet-style encoder–decoder with a small transformer bottleneck and a SAM-like mask head. Our ExecuTorch export is configured for a 512×512 input resolution and FP16 inference::</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Input</span><span class="w"> </span><span class="nx">Image</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">Prompts</span>
<span class="w">    </span><span class="nx">Image</span><span class="p">:</span><span class="w">  </span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Clicks</span><span class="p">:</span><span class="w"> </span><span class="p">(</span><span class="nx">encoded</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">extra</span><span class="w"> </span><span class="nx">channel</span><span class="p">)</span>
<span class="w">    </span><span class="nx">BBox</span><span class="p">:</span><span class="w">   </span><span class="p">(</span><span class="nx">encoded</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">extra</span><span class="w"> </span><span class="nx">channel</span><span class="p">)</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">Early</span><span class="o">-</span><span class="nx">Fusion</span><span class="w"> </span><span class="nx">Input</span><span class="w"> </span><span class="nx">Tensor</span>
<span class="w">    </span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">)</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nx">RGB</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">clicks</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">box</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">UNet</span><span class="w"> </span><span class="nx">Encoder</span><span class="w"> </span><span class="p">(</span><span class="nx">conv2d</span><span class="o">-</span><span class="nx">heavy</span><span class="p">)</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">DoubleConv</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">Downsampling</span><span class="w"> </span><span class="nx">blocks</span><span class="w"> </span><span class="p">(</span><span class="nx">strided</span><span class="w"> </span><span class="nx">conv2d</span><span class="p">)</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">Progressive</span><span class="w"> </span><span class="nx">channel</span><span class="w"> </span><span class="nx">growth</span><span class="w"> </span><span class="p">(</span><span class="nx">capped</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="mi">256</span><span class="w"> </span><span class="nx">channels</span><span class="p">)</span>
<span class="w">    </span><span class="err">└─</span><span class="w"> </span><span class="nx">BatchNorm</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">ReLU</span><span class="w"> </span><span class="nx">activations</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">Bottleneck</span><span class="w"> </span><span class="nx">Transformer</span><span class="w"> </span><span class="nx">Block</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">Flattened</span><span class="w"> </span><span class="nx">low</span><span class="o">-</span><span class="nx">resolution</span><span class="w"> </span><span class="nx">feature</span><span class="w"> </span><span class="nx">map</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="mi">1</span><span class="err">–</span><span class="mi">2</span><span class="w"> </span><span class="nx">Transformer</span><span class="w"> </span><span class="nx">layers</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="err">├─</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">Attention</span><span class="w"> </span><span class="p">(</span><span class="nx">GEMM</span><span class="p">)</span>
<span class="w">    </span><span class="err">│</span><span class="w">   </span><span class="err">└─</span><span class="w"> </span><span class="nx">MLP</span><span class="w"> </span><span class="nx">Projections</span><span class="w"> </span><span class="p">(</span><span class="nx">GEMM</span><span class="p">)</span>
<span class="w">    </span><span class="err">└─</span><span class="w"> </span><span class="nx">Global</span><span class="w"> </span><span class="nx">context</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">low</span><span class="w"> </span><span class="nx">spatial</span><span class="w"> </span><span class="nx">resolution</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">UNet</span><span class="w"> </span><span class="nx">Decoder</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">ConvTranspose</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">upsampling</span><span class="w"> </span><span class="nx">layers</span><span class="w"> </span><span class="p">(</span><span class="nx">conv2d</span><span class="p">)</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">Skip</span><span class="w"> </span><span class="nx">connections</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">encoder</span><span class="w"> </span><span class="nx">stages</span>
<span class="w">    </span><span class="err">└─</span><span class="w"> </span><span class="nx">DoubleConv</span><span class="w"> </span><span class="nx">blocks</span><span class="w"> </span><span class="nx">at</span><span class="w"> </span><span class="nx">each</span><span class="w"> </span><span class="nx">scale</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">Prompt</span><span class="o">-</span><span class="nx">Aware</span><span class="w"> </span><span class="nx">Mask</span><span class="w"> </span><span class="nx">Head</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">Lightweight</span><span class="w"> </span><span class="nx">decoder</span><span class="w"> </span><span class="nx">transformer</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">late</span><span class="o">-</span><span class="nx">fusion</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">point</span><span class="w"> </span><span class="nx">prompts</span>
<span class="w">    </span><span class="err">├─</span><span class="w"> </span><span class="nx">Mask</span><span class="w"> </span><span class="nx">MLP</span><span class="w"> </span><span class="nx">block</span><span class="w"> </span><span class="p">(</span><span class="nx">SAM</span><span class="o">-</span><span class="nx">style</span><span class="w"> </span><span class="nx">small</span><span class="w"> </span><span class="nx">MLPs</span><span class="w"> </span><span class="nx">over</span><span class="w"> </span><span class="nx">features</span><span class="p">)</span>
<span class="w">    </span><span class="err">└─</span><span class="w"> </span><span class="nx">Final</span><span class="w"> </span><span class="nx">projection</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">foreground</span><span class="w"> </span><span class="nx">mask</span><span class="w"> </span><span class="nx">logits</span>
<span class="w">    </span><span class="err">↓</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">Mask</span><span class="w"> </span><span class="nx">Logits</span><span class="w"> </span><span class="p">(</span><span class="nx">B</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span><span class="w"> </span><span class="mi">512</span><span class="p">)</span>
</code></pre></div>

<p><strong>Key Characteristics:</strong><br />
- <strong>Conv2d-Heavy</strong>: The UNet encoder–decoder stack is dominated by conv2d layers (DoubleConv down/upsampling blocks plus ConvTranspose for upsampling), making <strong>conv2d operations the dominant compute workload</strong> (~47% of execution time in optimized model).<br />
- <strong>Input Size</strong>: 512×512 pixels (smaller than EfficientSAM's 1024×1024, optimized for speed).<br />
- <strong>Precision</strong>: FP16 throughout (exported with FP16 precision for XNNPACK delegation).<br />
- <strong>Interpolation</strong>: Originally used bicubic interpolation for mask upsampling; optimized version uses bilinear (see <a href="#appendix-a-fp16-export--source-code-adjustments">Appendix A</a>).</p>
<h3><strong>Why Conv2d Performance Matters</strong></h3>
<p>SqueezeSAM's architecture is <strong>conv2d-heavy</strong> because:<br />
1. <strong>U-Net Encoder</strong>: 8 convolutional stages with patch embedding and downsampling layers<br />
2. <strong>Mask Decoder</strong>: Conv transpose operations for upsampling<br />
3. <strong>Conv2d Dominance</strong>: In the optimized bilinear model, CONV operations account for <strong>~47% of execution time</strong> (macOS, 1T, SME2-On), making iGEMM kernel performance critical</p>
<p>This makes SqueezeSAM an ideal candidate for evaluating FP16 iGEMM SME2 kernels, which accelerate conv2d operations through optimized matrix multiplication.</p>
<hr />
<h2><strong>3. Performance Measurement Setup &amp; Methodology</strong></h2>
<h3><strong>Hardware &amp; Software Configuration</strong></h3>
<ul>
<li><strong>Platforms</strong>: </li>
<li><strong>macOS</strong>: Apple M4 chip with ARM SME2 support</li>
<li><strong>Android</strong>: vivo X300 (Android 15, Armv9) with ARM SME2 support</li>
<li><strong>Framework</strong>: ExecuTorch with XNNPACK backend</li>
<li><strong>Testing</strong>: SME2-On vs SME2-Off configurations using 1 and 4 CPU threads</li>
<li><strong>Data Collection</strong>: Executor_runner timing, ETDump operator analysis, XNNTrace kernel profiling</li>
<li><strong>Inputs</strong>: Real prompt tensors (dog image + central clicks, ImageNet normalized)</li>
</ul>
<h3><strong>Analysis Methodology</strong></h3>
<ul>
<li><strong>End-to-End Timing</strong>: Executor_runner total execution time (100 warmup + 100 timed runs)</li>
<li><strong>Operator Analysis</strong>: ETDump Method::execute timing with operator categorization (CONV, GEMM, ELEMENTWISE, DATA_MOVEMENT, OTHER)</li>
<li><strong>Kernel Profiling</strong>: XNNTrace logs for SME2 kernel availability verification</li>
<li><strong>Robust Statistics</strong>: Median latency calculated from ETDump timing data</li>
</ul>
<h3><strong>Android Note</strong></h3>
<p>The Android SME2-off single-thread runner experiences intermittent segmentation faults due to a pthreadpool bug in the sequential execution path. Reported numbers are from successful runs and are reproducible.</p>
<hr />
<h2><strong>4. End-to-End Performance Results</strong></h2>
<h3><strong>Bicubic Model (Original Export)</strong></h3>
<h4><strong>macOS Performance</strong></h4>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Median E2E</th>
<th>SME2 Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2-Off - 1 CPU</strong></td>
<td>545.23 ms</td>
<td><strong>baseline (1.00×)</strong></td>
</tr>
<tr>
<td><strong>SME2-On - 1 CPU</strong></td>
<td>290.44 ms</td>
<td><strong>1.88×</strong></td>
</tr>
<tr>
<td><strong>SME2-Off - 4 CPU</strong></td>
<td>319.84 ms</td>
<td><strong>baseline (1.00×)</strong></td>
</tr>
<tr>
<td><strong>SME2-On - 4 CPU</strong></td>
<td>267.01 ms</td>
<td><strong>1.20×</strong></td>
</tr>
</tbody>
</table>
<h4><strong>Android Performance</strong></h4>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Median E2E</th>
<th>SME2 Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2-Off - 1 CPU</strong></td>
<td>1,800.10 ms</td>
<td><strong>baseline (1.00×)</strong></td>
</tr>
<tr>
<td><strong>SME2-On - 1 CPU</strong></td>
<td>974.59 ms</td>
<td><strong>1.85×</strong></td>
</tr>
<tr>
<td><strong>SME2-Off - 4 CPU</strong></td>
<td>1,012.73 ms</td>
<td><strong>baseline (1.00×)</strong></td>
</tr>
<tr>
<td><strong>SME2-On - 4 CPU</strong></td>
<td>866.24 ms</td>
<td><strong>1.18×</strong></td>
</tr>
</tbody>
</table>
<p><strong>Insight</strong>: Bicubic model shows <strong>1.85-1.88× SME2 speedup</strong>, but portable fallback operations (~200ms on macOS, ~760ms on Android) limit the total gain.</p>
<h3><strong>Bilinear Model (Optimized Export)</strong></h3>
<h4><strong>macOS Performance</strong></h4>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Median E2E</th>
<th>SME2 Speedup</th>
<th>vs Bicubic SME2-On</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2-Off - 1 CPU</strong></td>
<td>320.42 ms</td>
<td><strong>baseline (1.00×)</strong></td>
<td></td>
</tr>
<tr>
<td><strong>SME2-On - 1 CPU</strong></td>
<td>72.67 ms</td>
<td><strong>4.41×</strong></td>
<td><strong>4.00× faster</strong></td>
</tr>
<tr>
<td><strong>SME2-Off - 4 CPU</strong></td>
<td>98.71 ms</td>
<td><strong>baseline (1.00×)</strong></td>
<td></td>
</tr>
<tr>
<td><strong>SME2-On - 4 CPU</strong></td>
<td>50.60 ms</td>
<td><strong>1.95×</strong></td>
<td><strong>5.28× faster</strong></td>
</tr>
</tbody>
</table>
<h4><strong>Android Performance</strong></h4>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Median E2E</th>
<th>SME2 Speedup</th>
<th>vs Bicubic SME2-On</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2-Off - 1 CPU</strong></td>
<td>1,131.60 ms</td>
<td><strong>baseline (1.00×)</strong></td>
<td></td>
</tr>
<tr>
<td><strong>SME2-On - 1 CPU</strong></td>
<td>303.23 ms</td>
<td><strong>3.73×</strong></td>
<td><strong>3.21× faster</strong></td>
</tr>
<tr>
<td><strong>SME2-Off - 4 CPU</strong></td>
<td>337.07 ms</td>
<td><strong>baseline (1.00×)</strong></td>
<td></td>
</tr>
<tr>
<td><strong>SME2-On - 4 CPU</strong></td>
<td>194.12 ms</td>
<td><strong>1.75×</strong></td>
<td><strong>4.46× faster</strong></td>
</tr>
</tbody>
</table>
<p><strong>Insight</strong>: Bilinear model shows <strong>4.41× SME2 speedup on macOS</strong> and <strong>3.73× on Android</strong> (1T), demonstrating that <strong>replacing portable fallback operators with delegated operators</strong> enables the full benefit of SME2 kernels. The optimized model (with delegated operators instead of portable) is <strong>4.00× faster than bicubic SME2-On</strong> on macOS.</p>
<h3><strong>Performance Breakdown: SME2 vs Model Optimization</strong></h3>
<table>
<thead>
<tr>
<th>Optimization</th>
<th>macOS 1T Speedup</th>
<th>Android 1T Speedup</th>
<th>Source</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2 Kernels (bicubic)</strong></td>
<td>1.88×</td>
<td>1.85×</td>
<td>SME2 iGEMM/GEMM kernels</td>
</tr>
<tr>
<td><strong>Bilinear Optimization</strong></td>
<td>1.70× (SME2-Off)</td>
<td>1.84× (SME2-Off)</td>
<td>Eliminates portable fallback</td>
</tr>
<tr>
<td><strong>Combined (bilinear + SME2)</strong></td>
<td>4.41×</td>
<td>3.73×</td>
<td>Both optimizations</td>
</tr>
</tbody>
</table>
<p><strong>Key Finding</strong>: The <strong>4.41× speedup</strong> in the bilinear model comes from <strong>both</strong> SME2 kernels (1.88×) and model optimization (1.70×), with the combined effect being multiplicative when portable fallbacks are eliminated.</p>
<hr />
<h2><strong>5. Operator-Level Evidence (SME2 Hits CONV and GEMM)</strong></h2>
<h3><strong>Current SME2 Kernel Coverage</strong></h3>
<p>SME2 acceleration targets both <strong>GEMM</strong> (General Matrix Multiply) and <strong>iGEMM</strong> (Implicit GEMM for conv2d) operations. SqueezeSAM benefits from both:</p>
<ul>
<li><strong>iGEMM Kernels</strong>: Accelerate conv2d operations (dominant workload, ~47% of execution time)</li>
<li><strong>GEMM Kernels</strong>: Accelerate fully-connected and matmul operations (~2% of execution time)</li>
</ul>
<h3><strong>Operator Categories</strong></h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>Definition</th>
<th>ETDump Pattern</th>
<th>Backend</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CONV</strong></td>
<td>2D Convolution (conv2d)</td>
<td><code>Convolution</code>, <code>Conv</code></td>
<td>Delegated (XNNPACK iGEMM)</td>
</tr>
<tr>
<td><strong>GEMM</strong></td>
<td>General Matrix Multiply</td>
<td><code>Fully Connected</code>, <code>GEMM</code></td>
<td>Delegated (XNNPACK GEMM)</td>
</tr>
<tr>
<td><strong>ELEMENTWISE</strong></td>
<td>Point-wise operations</td>
<td><code>GELU</code>, <code>ReLU</code>, <code>Add</code>, <code>Mul</code></td>
<td>Delegated (XNNPACK)</td>
</tr>
<tr>
<td><strong>DATA_MOVEMENT</strong></td>
<td>Memory operations</td>
<td><code>Copy</code>, <code>Transpose</code></td>
<td>Delegated (XNNPACK)</td>
</tr>
<tr>
<td><strong>OTHER</strong></td>
<td>All other operations</td>
<td>Various (e.g., <code>native_call_index.Tensor_out</code>)</td>
<td>Mixed (Portable/Delegated)</td>
</tr>
</tbody>
</table>
<h3><strong>macOS Operator Breakdown (1T, Bilinear Model)</strong></h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>SME2-On (ms)</th>
<th>SME2-Off (ms)</th>
<th>Speedup</th>
<th>% of E2E (On)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CONV Operations</strong></td>
<td>34.4</td>
<td>277.6</td>
<td><strong>8.1×</strong></td>
<td>47.0%</td>
</tr>
<tr>
<td><strong>GEMM Operations</strong></td>
<td>1.4</td>
<td>8.4</td>
<td><strong>5.9×</strong></td>
<td>1.9%</td>
</tr>
<tr>
<td><strong>ELEMENTWISE Operations</strong></td>
<td>0.6</td>
<td>0.6</td>
<td>1.1×</td>
<td>0.8%</td>
</tr>
<tr>
<td><strong>DATA_MOVEMENT Operations</strong></td>
<td>14.5</td>
<td>13.6</td>
<td>0.9×</td>
<td>19.8%</td>
</tr>
<tr>
<td><strong>OTHER Operations</strong></td>
<td>22.4</td>
<td>22.8</td>
<td>1.0×</td>
<td>30.6%</td>
</tr>
<tr>
<td><strong>E2E Latency</strong></td>
<td><strong>73.25 ms</strong></td>
<td><strong>322.98 ms</strong></td>
<td><strong>4.41×</strong></td>
<td>-</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight</strong>: CONV operations show <strong>8.1× speedup</strong> with SME2 iGEMM kernels, transforming from 86% of execution time (SME2-Off) to 47% (SME2-On). GEMM operations show <strong>5.9× speedup</strong> with SME2 GEMM kernels.</p>
<h3><strong>Android Operator Breakdown (1T, Bilinear Model)</strong></h3>
<table>
<thead>
<tr>
<th>Category</th>
<th>SME2-On (ms)</th>
<th>SME2-Off (ms)</th>
<th>Speedup</th>
<th>% of E2E (On)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>CONV Operations</strong></td>
<td>97.7</td>
<td>846.4</td>
<td><strong>8.7×</strong></td>
<td>32.2%</td>
</tr>
<tr>
<td><strong>GEMM Operations</strong></td>
<td>7.0</td>
<td>29.6</td>
<td><strong>4.2×</strong></td>
<td>2.3%</td>
</tr>
<tr>
<td><strong>ELEMENTWISE Operations</strong></td>
<td>1.5</td>
<td>1.5</td>
<td>1.0×</td>
<td>0.5%</td>
</tr>
<tr>
<td><strong>DATA_MOVEMENT Operations</strong></td>
<td>110.2</td>
<td>116.0</td>
<td>1.1×</td>
<td>36.3%</td>
</tr>
<tr>
<td><strong>OTHER Operations</strong></td>
<td>87.2</td>
<td>84.0</td>
<td>1.0×</td>
<td>28.7%</td>
</tr>
<tr>
<td><strong>E2E Latency</strong></td>
<td><strong>303.61 ms</strong></td>
<td><strong>1,078.50 ms</strong></td>
<td><strong>3.55×</strong></td>
<td>-</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight</strong>: Android shows similar patterns with <strong>8.7× CONV speedup</strong> and <strong>4.2× GEMM speedup</strong>. The higher absolute latency is due to Android device performance characteristics.</p>
<h3><strong>Bicubic vs Bilinear: Operator Impact</strong></h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>CONV (ms)</th>
<th>GEMM (ms)</th>
<th>OTHER (ms)</th>
<th>E2E (ms)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Bicubic SME2-On</strong></td>
<td>34.8</td>
<td>1.4</td>
<td>237.9</td>
<td>291.71</td>
</tr>
<tr>
<td><strong>Bilinear SME2-On</strong></td>
<td>34.4</td>
<td>1.4</td>
<td>22.4</td>
<td>73.25</td>
</tr>
<tr>
<td><strong>Difference</strong></td>
<td>-0.4</td>
<td>0.0</td>
<td><strong>-215.5</strong></td>
<td><strong>-218.46</strong></td>
</tr>
</tbody>
</table>
<p><strong>Key Finding</strong>: The bilinear optimization <strong>eliminated ~216ms of OTHER operations</strong> (portable fallback), while CONV and GEMM operations remained nearly identical. This confirms that the speedup comes from <strong>model optimization</strong>, not kernel changes.</p>
<p><strong>DATA_MOVEMENT Insight</strong>: The 110.2 ms of DATA_MOVEMENT operations (36.3% of Android 1T Bilinear Model E2E) is primarily <strong>memory layout conversions</strong> between NCHW (PyTorch default) and NHWC (XNNPACK optimized) formats. Transpose operations dominate (85.1% of DATA_MOVEMENT time), with two main operations accounting for 80.4% of all DATA_MOVEMENT time:<ul>
<li><code>Transpose (ND, X16) #1</code>:67.77 ms (NCHW → NHWC before convolutions)</li>
<li><code>Transpose (ND, X16) #2</code>:30.00 ms (NHWC → NCHW after convolutions)</li>
</ul></p>
<p><strong>Why This Happens</strong>: Throughout the UNet encoder-decoder, each convolution block follows this pattern:</p>
<div class="codehilite"><pre><span></span><code>BatchNorm/GroupNorm (NCHW) → Transpose #1 (NCHW→NHWC) → Convolution (NHWC) → Transpose #2 (NHWC→NCHW) → BatchNorm/GroupNorm (NCHW)
</code></pre></div>

<p>XNNPACK's convolution kernels are optimized for NHWC layout (better cache locality, SIMD efficiency), while PyTorch normalization layers (<code>BatchNorm2d</code>, <code>GroupNorm</code>) require NCHW format. This layout conversion overhead is a <strong>necessary trade-off</strong> that enables the <strong>8-9× speedup</strong> on convolution operations.</p>
<h3><strong>Int8 Quantized Model Performance (Bilinear, 1 CPU)</strong></h3>
<p>The Int8 quantized model (Q8) shows similar optimization benefits with bilinear interpolation:</p>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Median E2E</th>
<th>SME2 Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SME2-Off - 1 CPU</strong></td>
<td>553.10 ms</td>
<td><strong>baseline (1.00×)</strong></td>
</tr>
<tr>
<td><strong>SME2-On - 1 CPU (w/o Int8 iGEMM)</strong></td>
<td>294.21 ms</td>
<td><strong>1.88×</strong></td>
</tr>
</tbody>
</table>
<p><strong>Important Note</strong>: The current SME2-On results for Int8 are achieved <strong>without dedicated Int8 iGEMM SME2 kernels</strong>. The model currently uses GEMM transformation with FP32 SME2 kernels. ARM is actively working on integrating the developed Int8 iGEMM SME2 kernels into XNNPACK. Once these kernels are integrated, the actual SME2 Int8 inference time will be <strong>further improved</strong>, as the dedicated Int8 iGEMM kernels will provide more efficient acceleration for quantized convolution operations.</p>
<hr />
<h2><strong>6. Kernel Analysis (XNNPACK Delegation Verification)</strong></h2>
<h3><strong>SqueezeSAM FP16 GEMM/IGEMM Kernel View (Bilinear Model)</strong></h3>
<table>
<thead>
<tr>
<th>Kernel Name</th>
<th>SME2-On</th>
<th>SME2-Off</th>
<th>Architecture</th>
<th>Delegation</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>xnn_pf16_f16_igemm_minmax_fp16_ukernel_32x32c2__neonsme2</code></td>
<td>✅ <strong>Present</strong></td>
<td>❌ <strong>Absent</strong></td>
<td>SME2</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_pf16_gemm_minmax_ukernel_1x32c2__neonsme2</code></td>
<td>✅ <strong>Present</strong></td>
<td>❌ <strong>Absent</strong></td>
<td>SME2</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_pf16_gemm_minmax_ukernel_32x32c2__neonsme2</code></td>
<td>✅ <strong>Present</strong></td>
<td>❌ <strong>Absent</strong></td>
<td>SME2</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_f16_igemm_minmax_ukernel_1x16__asm_aarch64_neonfp16arith_ld64</code></td>
<td>❌ <strong>Absent</strong></td>
<td>✅ <strong>Present</strong></td>
<td>NEON</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_f16_igemm_minmax_ukernel_6x16__asm_aarch64_neonfp16arith_cortex_a75</code></td>
<td>❌ <strong>Absent</strong></td>
<td>✅ <strong>Present</strong></td>
<td>NEON</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_f16_gemm_minmax_ukernel_1x16__asm_aarch64_neonfp16arith_ld64</code></td>
<td>❌ <strong>Absent</strong></td>
<td>✅ <strong>Present</strong></td>
<td>NEON</td>
<td>XNNPACK Delegated</td>
</tr>
<tr>
<td><code>xnn_f16_gemm_minmax_ukernel_6x16__asm_aarch64_neonfp16arith_cortex_a75</code></td>
<td>❌ <strong>Absent</strong></td>
<td>✅ <strong>Present</strong></td>
<td>NEON</td>
<td>XNNPACK Delegated</td>
</tr>
</tbody>
</table>
<h3><strong>Key Observations</strong></h3>
<ol>
<li>
<p><strong>SME2 iGEMM Kernel</strong>: <code>xnn_pf16_f16_igemm_minmax_fp16_ukernel_32x32c2__neonsme2</code> from <a href="https://github.com/google/XNNPACK/pull/8687">PR #8687</a> is <strong>present when SME2 is enabled</strong>, providing the <strong>8.1-8.7× CONV speedup</strong> observed in operator breakdowns.</p>
</li>
<li>
<p><strong>SME2 GEMM Kernels</strong>: <code>xnn_pf16_gemm_minmax_ukernel_*__neonsme2</code> are <strong>present when SME2 is enabled</strong>, providing the <strong>4-6× GEMM speedup</strong> observed in operator breakdowns.</p>
</li>
<li>
<p><strong>NEON Fallback Kernels</strong>: When SME2 is disabled, XNNPACK falls back to NEON FP16 kernels (<code>xnn_f16_*__asm_aarch64_neonfp16arith_*</code>), which are slower but still delegated.</p>
</li>
<li>
<p><strong>Kernel Visibility</strong>: The iGEMM kernel is now <strong>visible in xnntrace logs</strong> after adding the <code>XNN_INIT_HMP_PACKED_IGEMM_UKERNEL</code> logging macro (see <a href="#appendix-a-fp16-export--source-code-adjustments">Appendix A</a>).</p>
</li>
<li>
<p><strong>Platform Consistency</strong>: macOS and Android use the <strong>same kernels</strong>, confirming consistent XNNPACK behavior across platforms.</p>
</li>
</ol>
<hr />
<h2><strong>7. Configuration Details</strong></h2>
<h3><strong>macOS Configurations</strong></h3>
<ul>
<li><strong>SME2-On Runner</strong>: <code>release-base-f16igemm</code> (FP16 iGEMM kernels, SME2 enabled)</li>
<li><strong>SME2-Off Runner</strong>: <code>release-base-f16igemm-sme2-off-nomarch</code> (FP16 iGEMM kernels, SME2 disabled, NaN fix applied)</li>
<li><strong>Runs</strong>: 100 timing runs, 50 warmup runs per configuration</li>
<li><strong>Inputs</strong>: Real prompt tensors (ImageNet normalized)</li>
</ul>
<h3><strong>Android Configurations</strong></h3>
<ul>
<li><strong>SME2-On Runner</strong>: <code>android-arm64-v9a-release-f16igemm</code> (FP16 iGEMM kernels, SME2 enabled)</li>
<li><strong>SME2-Off Runner</strong>: <code>android-arm64-v9a-release-f16igemm-sme2-off</code> (FP16 iGEMM kernels, SME2 disabled, NaN fix applied)</li>
<li><strong>Runs</strong>: 100 timing runs, 50 warmup runs per configuration</li>
<li><strong>Inputs</strong>: Real prompt tensors (ImageNet normalized)</li>
<li><strong>Note</strong>: Android SME2-Off 1T runner experiences intermittent crashes; reported numbers are from successful runs</li>
</ul>
<hr />
<h2 id="appendix-a-fp16-export--source-code-adjustments"><strong>Appendix A: FP16 Export &amp; Source Code Adjustments</strong></h2>
<h3><strong>FP16 Export Challenges</strong></h3>
<p>Exporting SqueezeSAM to FP16 ExecuTorch format required addressing several issues to ensure correct execution and optimal performance:</p>
<h4><strong>1. FP16 Accumulation Overflow (Fixed)</strong></h4>
<p><strong>Problem</strong>: The portable <code>native_group_norm</code> kernel used FP16 accumulation for large reduction dimensions (~16M elements), causing overflow (<code>inf</code>) that yielded NaNs when computing variance.</p>
<p><strong>Root Cause</strong>: When SME2 is disabled, ExecuTorch falls back to portable kernels for operations not covered by the FP16 delegate. The portable group-norm kernel stored <code>reduce_add</code> / <code>vec_powerf</code> results directly into FP16 buffers, causing overflow.</p>
<p><strong>Solution</strong>: Modified <code>native_group_norm</code> to accumulate statistics in <strong>double precision</strong> before converting to FP16 outputs:</p>
<div class="codehilite"><pre><span></span><code><span class="c1">// Before (caused overflow):</span>
<span class="n">CTYPE</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">reduce_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">inner_size</span><span class="p">);</span>
<span class="n">CTYPE</span><span class="w"> </span><span class="n">sq_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vec_powerf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">inner_size</span><span class="p">);</span>

<span class="c1">// After (fixed):</span>
<span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">reduce_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">inner_size</span><span class="p">));</span>
<span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">sq_sum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">(</span><span class="n">vec_powerf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">inner_size</span><span class="p">));</span>
</code></pre></div>

<p><strong>Impact</strong>: Eliminated NaN outputs in SME2-off runs, enabling accurate performance comparisons.</p>
<h4><strong>2. Bicubic Interpolation Fallback (Optimized)</strong></h4>
<p><strong>Problem</strong>: The original model used <code>F.interpolate(..., mode="bicubic")</code> for mask upsampling, which ExecuTorch/XNNPACK does not support. This caused fallback to portable <code>aten.index.Tensor_out</code> operations, consuming <strong>~200ms per run</strong> (~81% of execution time in bicubic model).</p>
<p><strong>What is Bicubic vs Bilinear?</strong><br />
- <strong>Bicubic interpolation</strong>: Uses a 4×4 pixel neighborhood with cubic weighting, producing smoother results but requiring more computation. Not supported by XNNPACK.<br />
- <strong>Bilinear interpolation</strong>: Uses a 2×2 pixel neighborhood with linear weighting, faster and supported by XNNPACK. Produces slightly less smooth results but sufficient for segmentation tasks.</p>
<p><strong>Solution</strong>: Replaced bicubic with bilinear interpolation in the model code:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Before (bicubic, falls back to portable):</span>
<span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bicubic&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># After (bilinear, delegated to XNNPACK):</span>
<span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p><strong>Impact</strong>: <br />
- Eliminated ~200ms of portable kernel execution<br />
- Kept entire graph within XNNPACK delegation<br />
- Enabled clean SME2 vs NEON kernel comparison<br />
- <strong>4.00× faster</strong> than bicubic on macOS (SME2-On, 1T)</p>
<h4><strong>3. FP16 iGEMM Kernel Logging (Enhanced)</strong></h4>
<p><strong>Problem</strong>: The FP16 iGEMM SME2 kernel (<code>xnn_pf16_f16_igemm_minmax_fp16_ukernel_32x32c2__neonsme2</code>) from <a href="https://github.com/google/XNNPACK/pull/8687">XNNPACK PR #8687</a> was not visible in xnntrace logs, making it impossible to verify kernel usage.</p>
<p><strong>Root Cause</strong>: The kernel was initialized using <code>xnn_init_hmp_packed_igemm_ukernel()</code> directly, which does not emit log messages, unlike other kernels that use logging macros.</p>
<p><strong>Solution</strong>: Created a new macro <code>XNN_INIT_HMP_PACKED_IGEMM_UKERNEL</code> that includes logging:</p>
<div class="codehilite"><pre><span></span><code><span class="cp">#define XNN_INIT_HMP_PACKED_IGEMM_UKERNEL(ukernel)                    \</span>
<span class="cp">  xnn_init_hmp_packed_igemm_ukernel(                                 \</span>
<span class="cp">      (xnn_packed_lhs_igemm_ukernel_fn)ukernel);                     \</span>
<span class="cp">  xnn_log_info(&quot;Using igemm microkernel &#39;%s&#39;.&quot;, #ukernel);</span>
</code></pre></div>

<p><strong>Impact</strong>: Enabled automatic kernel extraction from xnntrace logs, making the iGEMM kernel visible in kernel view tables and performance reports.</p>
<h3><strong>XNNPACK PR #8687: FP16 iGEMM SME2 Kernels</strong></h3>
<p>This analysis leverages the <strong>FP16 iGEMM SME2 microkernels</strong> introduced in <a href="https://github.com/google/XNNPACK/pull/8687">XNNPACK PR #8687</a>. These kernels provide:</p>
<ul>
<li><strong>Packed FP16 iGEMM</strong>: <code>xnn_pf16_f16_igemm_minmax_fp16_ukernel_32x32c2__neonsme2</code> for conv2d operations</li>
<li><strong>SME2 Architecture</strong>: Optimized for ARM Scalable Matrix Extension 2 (SME2) with 32×32c2 tile sizes</li>
<li><strong>Conv2d Acceleration</strong>: Direct acceleration of convolution operations through optimized matrix multiplication</li>
</ul>
<p>These kernels are critical for SqueezeSAM's performance because the model is <strong>conv2d-heavy</strong>, with CONV operations accounting for ~47% of execution time in the optimized model.</p>
<hr />
<h2><strong>Appendix B: Identifying the Bicubic Interpolation Bottleneck</strong></h2>
<p>This appendix describes the methodology used to identify and resolve the bicubic interpolation bottleneck that was consuming ~200ms per run.</p>
<h3><strong>Step 1: Inspect ETDump Operator Analysis</strong></h3>
<p>The first clue came from analyzing operator-level performance data. The ETDump operator CSV analysis flagged <code>native_call_index.Tensor_out</code> as the top "OTHER" operation, consuming approximately <strong>~200ms per run</strong>. This was significantly higher than expected and indicated a fallback to portable (non-delegated) kernels.</p>
<h3><strong>Step 2: Examine the Exported FX Graph</strong></h3>
<p>To understand where this operation originated, we examined the exported FX graph representation of the model. The graph showed node 1363 (and 15 sibling nodes) invoking <code>aten.index.Tensor_out</code> operations immediately after the decoder output. These index operations are typically generated when PyTorch's <code>F.interpolate()</code> function cannot be directly delegated to XNNPACK.</p>
<h3><strong>Step 3: Trace Back to Model Source Code</strong></h3>
<p>Matching the FX graph nodes back to the original model source code revealed that these <code>aten.index.Tensor_out</code> calls originated from <code>F.interpolate(..., mode="bicubic")</code> in the mask decoder. The bicubic interpolation mode is not supported by XNNPACK (which only supports bilinear and nearest-neighbor interpolation), causing ExecuTorch to fall back to portable PyTorch operations.</p>
<p><strong>Location in model code</strong>: The bicubic interpolation was used in the mask upsampling step, where the decoder output masks are resized to the final output resolution.</p>
<h3><strong>Step 4: Apply the Fix and Verify</strong></h3>
<p>The fix involved replacing bicubic interpolation with bilinear interpolation:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Before (bicubic, falls back to portable):</span>
<span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bicubic&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># After (bilinear, delegated to XNNPACK):</span>
<span class="n">F</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">),</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>

<p>After re-exporting the model and rerunning the performance analysis:<br />
- The <code>native_call_index.Tensor_out</code> operations disappeared from the operator breakdown<br />
- The "OTHER" category dropped from ~238ms to ~22ms (macOS, 1T, SME2-On)<br />
- End-to-end latency improved from ~291ms to ~73ms (macOS, 1T, SME2-On)<br />
- The entire graph remained within XNNPACK delegation, enabling clean SME2 vs NEON kernel comparison</p>
<h3><strong>Key Takeaways</strong></h3>
<ol>
<li>
<p><strong>SME2 acceleration is critical for compute-heavy operations</strong>: The <strong>8-9× speedup</strong> on CONV operations (via FP16 iGEMM SME2 kernels) and <strong>4-6× speedup</strong> on GEMM operations demonstrate that SME2 is essential for accelerating compute-intensive workloads. For conv2d-heavy models like SqueezeSAM, iGEMM performance directly determines overall inference speed.</p>
</li>
<li>
<p><strong>Memory-bound and control operations must also be optimized</strong>: While SME2 accelerates compute-heavy operations (GEMM/iGEMM), memory-bound operations (e.g., data movement, transpose, memory layout conversions) and unsupported operators can become bottlenecks. Operator-level analysis (ETDump breakdowns) and FX graph inspection help identify portable fallbacks. Replacing unsupported operations (bicubic → bilinear) and ensuring full delegate coverage eliminates these bottlenecks and enables accurate SME2 vs NEON comparisons.</p>
</li>
<li>
<p><strong>Operator-level analysis is critical</strong>: ETDump operator breakdowns can quickly identify unexpected portable fallbacks that limit performance, enabling targeted optimization of both compute-heavy and memory-bound operations.</p>
</li>
</ol>
<hr />
<p><strong>Report Generated</strong>: November 14, 2025</p>

</html>